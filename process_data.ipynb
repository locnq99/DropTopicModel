{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0869de74-b4ea-4552-a1fd-bd721052235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pickle \n",
    "from tqdm import tqdm \n",
    "import os\n",
    "from scipy import sparse \n",
    "import gc\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02ce798-9024-4bf9-8954-9b1e15d21bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = '/data/datn/final_data/holdout_SOLA-TPS-idrop-nograd-nobonus/SOLA-TPS-idrop-nograd-nobonus/dataset/6-statictarget-datasets/'\n",
    "path_out = '/data/datn/final_data/holdout_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba014b5-ff79-495d-b381-abbdbad63f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yahoo',\n",
       " 'TMN',\n",
       " 'TMNtitle',\n",
       " 'Grolier',\n",
       " 'Agnews-title',\n",
       " 'NYtimes',\n",
       " 'Agnews',\n",
       " 'Twitter',\n",
       " '20newgroups']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_dataset = os.listdir(path_in)\n",
    "lst_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871f3cea-7f8d-4e50-ad14-cd639ce68dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bow(path_bow, vocab_len):\n",
    "    with open(path_bow, 'r') as f:\n",
    "        data = f.read().splitlines()\n",
    "    sparse_vector = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        dense_vector = np.zeros(vocab_len, dtype = np.int32)\n",
    "        terms = data[i].split()[1:]\n",
    "        for j in range(len(terms)):\n",
    "            idx, cnt = terms[j].split(':')\n",
    "            dense_vector[int(idx)] = int(cnt)\n",
    "        sparse_vector.append(sparse.csr_matrix(dense_vector))\n",
    "    return vstack(sparse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb3748d-df7f-4493-9f87-28451875ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prior_vector(prior):\n",
    "    prior_vector = []\n",
    "    for i in tqdm(range(len(prior))):\n",
    "        prior_vector.append(prior[i].split())\n",
    "    prior_vector = np.array(prior_vector, dtype = np.float64)\n",
    "    return prior_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3016abf0-211a-431e-b5e3-95a300dd98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(data, path, is_pickle = True):\n",
    "    if is_pickle: \n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump(data, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path,'w') as f:\n",
    "            f.write('\\n'.join(data))\n",
    "def read_file(path):\n",
    "    with open(path,'r') as f:\n",
    "        data = f.read().splitlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c92e930f-a6f9-4f6d-aa00-d3a155437561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path_in, path_out, dataset):\n",
    "    lst_file = os.listdir(path_in + dataset)\n",
    "    # create path dataset out \n",
    "    if not os.path.exists(path_out + dataset):\n",
    "        os.mkdir(path_out + dataset)\n",
    "    vocab = read_file(path_in + dataset + '/vocab.txt')\n",
    "    setting = read_file(path_in + dataset + '/setting.txt')\n",
    "    write_file(data = vocab,\n",
    "              path = path_out + dataset + '/vocab.txt',\n",
    "              is_pickle = False)\n",
    "    write_file(data = setting,\n",
    "              path = path_out + dataset + '/setting.txt',\n",
    "              is_pickle = False)\n",
    "    \n",
    "    for f in lst_file: \n",
    "        if 'train' in f or 'test' in f: \n",
    "            sparse_vector = convert_to_bow(path_bow = path_in + dataset + '/' + f,\n",
    "                                          vocab_len = len(vocab))\n",
    "            write_file(data = sparse_vector, \n",
    "                      path = path_out + dataset + '/' + f.split('.')[0] + '.pkl',\n",
    "                      is_pickle = True)\n",
    "            del sparse_vector\n",
    "            _ = gc.collect()\n",
    "        elif 'prior' in f:\n",
    "            prior = read_file(path_in + dataset + '/' + f)\n",
    "            prior = convert_prior_vector(prior)\n",
    "            write_file(data = prior,\n",
    "                      path = path_out + dataset + '/' + f.split('.')[0] + '.pkl',\n",
    "                      is_pickle = True)\n",
    "            del prior\n",
    "        # elif 'test' in f:\n",
    "        #     test = read_file(path_in + dataset + '/' + f)\n",
    "        #     write_file(data = test,\n",
    "        #               path = path_out + dataset + '/' + f.split('.')[0] + '.txt',\n",
    "        #               is_pickle = False)\n",
    "        #     _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8810be63-07cc-408f-82ab-53750b3423a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process dataset:  Yahoo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10000/10000 [00:02<00:00, 4475.32it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 4455.58it/s]\n",
      "100%|██████████████████████████████████| 21439/21439 [00:00<00:00, 62775.58it/s]\n",
      "100%|█████████████████████████████████| 517770/517770 [01:49<00:00, 4724.40it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5089.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process dataset:  TMN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6165.74it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5593.58it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5816.76it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5555.68it/s]\n",
      "100%|██████████████████████████████████| 11599/11599 [00:00<00:00, 76575.52it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5998.36it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6012.80it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5957.58it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5794.11it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6267.63it/s]\n",
      "100%|███████████████████████████████████| 31604/31604 [00:05<00:00, 5678.70it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5939.93it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6202.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process dataset:  TMNtitle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7754.95it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7460.82it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7574.27it/s]\n",
      "100%|███████████████████████████████████| 2823/2823 [00:00<00:00, 101929.34it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7804.84it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7816.39it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7180.32it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7663.39it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7215.13it/s]\n",
      "100%|███████████████████████████████████| 26251/26251 [00:03<00:00, 7320.93it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7028.47it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7719.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process dataset:  Grolier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 4852.73it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5448.22it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5028.59it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5558.21it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5101.90it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5148.17it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5317.62it/s]\n",
      "100%|██████████████████████████████████| 15269/15269 [00:00<00:00, 77264.20it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 4836.95it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5574.54it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 4966.90it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5215.77it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5500.88it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5361.34it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 4734.14it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5653.31it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5141.09it/s]\n",
      "100%|███████████████████████████████████| 23044/23044 [00:04<00:00, 4708.24it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 4794.81it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5461.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process dataset:  Agnews-title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 5568.88it/s]\n",
      "100%|██████████████████████████████████| 15936/15936 [00:00<00:00, 76603.55it/s]\n",
      "100%|█████████████████████████████████| 108401/108401 [00:19<00:00, 5537.22it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 5712.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process dataset:  Agnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10000/10000 [00:02<00:00, 4276.72it/s]\n",
      "100%|██████████████████████████████████| 32483/32483 [00:00<00:00, 75470.50it/s]\n",
      "100%|█████████████████████████████████| 110000/110000 [00:26<00:00, 4152.98it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:02<00:00, 4268.19it/s]\n"
     ]
    }
   ],
   "source": [
    "lst_dataset_use = ['Agnews', 'Agnews-title','TMN','TMNtitle',\\\n",
    "                      'Yahoo', 'Grolier']\n",
    "# lst_dataset_use = ['20newgroups']\n",
    "# lst_dataset_use = ['Agnews', 'TMN','20newgroups']\n",
    "for dataset in lst_dataset:\n",
    "    if dataset in lst_dataset_use:\n",
    "        print('Process dataset: ', dataset)\n",
    "        process_data(path_in, path_out, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97041a17-9013-42cd-8c88-568aeb280b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e63c9f6-4b54-4069-b904-1a7e04011e4b",
   "metadata": {},
   "source": [
    "# get docs embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "582d0baf-d49c-4be6-aa2f-17a91a9a990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "from tqdm import tqdm \n",
    "import gc\n",
    "from scipy import sparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af2f6673-e111-4ff0-b51a-c256fd56ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = '/data/datn/final_data/holdout_data/'\n",
    "lst_data = ['Agnews', 'Agnews-title','TMN','TMNtitle',\\\n",
    "                      'Yahoo', 'Grolier']\n",
    "# lst_data = ['20newgroups']\n",
    "lst_path = [path_folder + f for f in lst_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c2c1937-12c3-43d4-9394-de7e32aaf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "def get_docs_vector(prior, bows):\n",
    "    docs_vector = []\n",
    "    for i in tqdm(range(bows.shape[0])):\n",
    "        bow = bows[i].toarray().squeeze()\n",
    "        idx = bow.nonzero()[0]\n",
    "        cnt = bow[idx]\n",
    "        word_idx_appear = []\n",
    "        for j in range(len(idx)):\n",
    "            word_idx_appear += [idx[j]]* cnt[j]\n",
    "        if len(word_idx_appear) == 0:\n",
    "            vector = np.zeros(200)\n",
    "        else:\n",
    "            vector = prior[word_idx_appear]\n",
    "            vector = np.mean(vector, axis = 0)\n",
    "        docs_vector.append(vector)\n",
    "    docs_vector = np.array(docs_vector)\n",
    "    return docs_vector\n",
    "\n",
    "def write_data(path, data):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def process_docs_vector(path):\n",
    "    prior = read_data(path + '/prior.pkl')\n",
    "    \n",
    "    lst_file = os.listdir(path)\n",
    "    for f in lst_file:\n",
    "        if 'train' in f or 'part_1' in f:\n",
    "            bows = read_data(path + '/' + f)\n",
    "            docs_vector = get_docs_vector(prior, bows)\n",
    "            write_data(path + '/'+  f.split('.')[0] + '_vector.pkl', docs_vector)\n",
    "    del prior, bows, docs_vector\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0010f42-200c-41da-b7ad-e51ea9c8929c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process data: /data/datn/final_data/holdout_data/Agnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 110000/110000 [00:17<00:00, 6345.69it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 6417.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process data: /data/datn/final_data/holdout_data/Agnews-title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 108401/108401 [00:12<00:00, 8699.59it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 8911.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process data: /data/datn/final_data/holdout_data/TMN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 8351.46it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7767.78it/s]\n",
      "100%|███████████████████████████████████| 31604/31604 [00:04<00:00, 7423.72it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 8171.62it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 8239.89it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7491.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process data: /data/datn/final_data/holdout_data/TMNtitle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1000/1000 [00:00<00:00, 11484.64it/s]\n",
      "100%|████████████████████████████████████| 1000/1000 [00:00<00:00, 10467.78it/s]\n",
      "100%|██████████████████████████████████| 26251/26251 [00:02<00:00, 11040.08it/s]\n",
      "100%|████████████████████████████████████| 1000/1000 [00:00<00:00, 10516.73it/s]\n",
      "100%|████████████████████████████████████| 1000/1000 [00:00<00:00, 11180.64it/s]\n",
      "100%|████████████████████████████████████| 1000/1000 [00:00<00:00, 11309.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process data: /data/datn/final_data/holdout_data/Yahoo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 517770/517770 [01:06<00:00, 7841.54it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 7999.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process data: /data/datn/final_data/holdout_data/Grolier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5638.59it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6115.63it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6321.20it/s]\n",
      "100%|███████████████████████████████████| 23044/23044 [00:03<00:00, 5911.71it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6440.54it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6502.74it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6393.55it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6309.13it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6232.79it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 6373.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for path_data in lst_path:\n",
    "    print('process data:', path_data)\n",
    "    process_docs_vector(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4ef7f-c3c4-4c94-bde7-983f0c1b3f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd2338ff-f8d8-4e96-bee3-87a21e552e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "248fd3a0-c2c3-4ad4-aad4-c9bad2a817fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/datn/final_data/holdout_data/TMNtitle/data_test_1_part_1.pkl','rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a711e2e-80af-4e7a-87fc-2b9b4d604cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data[0].toarray()[0].nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d781495-aeec-4c7e-87fc-1794fbfff115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].toarray()[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444289e7-aefe-4707-8d1b-f9ae8f1d4b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
